Using sklearn 
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the diabetes dataset
diabetes = load_diabetes()
x  = diabetes.data
y = diabetes.target
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Model Building
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2_score(y_test, y_pred)}")
# Print the params
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")


manual 

import numpy as np
import pandas as pd
import seaborn as sns
# Multivarite Linear Regression using the normal equation.
x=np.array([[1,100,3],[1,300,7],[1,500,4]]) # features
y=np.array([300,700,600]) # target

# estimating parameters
xt=x.T
xtx=xt@x
xinv=np.linalg.inv(xtx)
xty=xt@y
b=xinv@xty # using the normal equation

print(f"""
Intercept: {b[0]}
Coefficient of 1st feature: {b[1]}
Coefficient of 2nd feature: {b[2]}""")

x_new = [1, 450, 8]
y_pred = x_new@b
print("Predicted value ", y_pred)


Gradient 

import numpy as np
import matplotlib.pyplot as plt

# Data (let's assume we have 2 features, x1 and x2)
X = np.array([[1, 1, 3],
              [1, 2, 6],
              [1, 3, 9],
              [1, 4, 12],
              [1, 5, 15]])  # First column is bias term (all 1s), next columns are features
y = np.array([2, 2.5, 3.5, 4.0, 4.5])

# Hyperparameters
w = np.array([0.5, 0.5, 0.5])  # Initial weights for bias, w1, w2 (for two features)
learning_rate = 0.01
epochs = 1000

m = len(X)
loss_history = []

# Gradient Descent
for epoch in range(epochs):
    # Predicted values using the model
    y_pred = X @ w  # X is (m x n) and w is (n x 1), result is (m x 1)

    # Compute the gradients
    dw = -(1/m) * X.T @ (y - y_pred)

    # Update weights
    w -= learning_rate * dw
    loss = np.mean((y - y_pred) ** 2)  # Mean Squared Error
    loss_history.append(loss)

    if epoch % 100 == 0:
        print(f'Epoch {epoch}: Loss = {loss:.3f}, w = {w}')

# Final weights after training
print(f'Final weights: w = {w}')

# Plot the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')
plt.legend()
plt.grid()
plt.show()
